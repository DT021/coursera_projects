---
title: "Exploratory text analysis (Milestone Report)"
author: "Tyler Burleigh"
date: "6/30/2019"
output: html_document
---

# Executive summary

The purpose of this document is to explore the data and generate insights that will
be useful to later developing a predictive text model. To that end, in this document
I will:

- Load the text data and convert it into a tidy format that can be analyzed using
the `tidytext` package for R
- Look at line and word counts
- Tokenize the texts into words and look at word frequencies
- Detect words containing non-English characters
- Test an approach to reducing the memory/compute footprint using random sub-sampling
- Tokenize the texts into ngrams and look at ngram frequencies

In summary random sub-sampling will be critical to minimizing memory/compute needs, 
and detecting non-English characters will be useful to data cleaning efforts. As
a first approach to creating a simple predictive model, I can use bigram frequencies.

The data reveals some differences between the three datasets, which suggests that
twitter / news / blog can be used as a contextual factor to improve prediction accuracy.s

# Load libraries and data
Load libraries and data files.

## Load libraries
```{r, message = FALSE, warning = FALSE}
suppress <- function(x){suppressMessages(suppressWarnings(x))}
suppress(library(tidyverse))
suppress(library(tidytext))
suppress(library(data.table))
suppress(library(kableExtra))

twitter <- readLines(file("data/en_US.twitter.txt",open="r"))
news <- readLines(file("data/en_US.news.txt",open="r"))
blogs <- readLines(file("data/en_US.blogs.txt",open="r"))

# Convert to tibbles
tibble(line = 1:length(twitter), text = twitter) -> twitter_t
tibble(line = 1:length(news), text = news) -> news_t
tibble(line = 1:length(blogs), text = blogs) -> blogs_t

rm(twitter)
rm(news)
rm(blogs)
```

# Tokenize
```{r, message = FALSE, warning = FALSE}
texts_tokenized <- list('twitter' = twitter_t %>% unnest_tokens(word, text), 
                        'news' = news_t %>% unnest_tokens(word, text), 
                        'blogs' = blogs_t %>% unnest_tokens(word, text))
```

# Basic summary
I'll start by looking at line and word counts.

## Line and word counts
```{r}
df <- data.frame(`lines` = numeric(), `words` = numeric(), `unique words` = numeric()) 

for(i in 1:3){
  df[i,1] <- max(texts_tokenized[[i]]$line)
  df[i,2] <- length(texts_tokenized[[i]]$word)
  df[i,3] <- length(unique(texts_tokenized[[i]]$word))
}
row.names(df) <- c("twitter", "news", "blogs")
df
```


# Word frequency
What are the most frequent words are in each dataset?

Because the datasets differ in total lines and word, I'll use word percentage
out of the total words in the dataset as a measure of frequency.

## Compute word frequencies
```{r}
wf_tbl <- list('twitter' = texts_tokenized$twitter %>% count(word, sort = TRUE) %>% mutate(pct = (n / sum(n)) * 100),
               'news' = texts_tokenized$news %>% count(word, sort = TRUE) %>% mutate(pct = (n / sum(n)) * 100),
               'blogs' = texts_tokenized$blogs %>% count(word, sort = TRUE) %>% mutate(pct = (n / sum(n)) * 100))
```

## Top 10 most common words
```{r}
kable(list(data.frame('twitter' = wf_tbl$twitter[1:10,c(1,3)]),
           data.frame('news' = wf_tbl$news[1:10,c(1,3)]),
           data.frame('blogs' = wf_tbl$blogs[1:10,c(1,3)])))
```

The most frequent words are similar across datasets

## Word frequency summary
```{r}
df <- data.frame(`mean word frequency` = numeric(), `max word frequency` = numeric(),
                 `mean word freq as pct` = numeric(), `max word freq as pct` = numeric()) 

for(i in 1:3){
  df[i,1] <- round(mean(wf_tbl[[i]]$n))
  df[i,2] <- max(wf_tbl[[i]]$n)
  df[i,3] <- round(mean(wf_tbl[[i]]$pct), 3)
  df[i,4] <- round(max(wf_tbl[[i]]$pct), 3)
}
row.names(df) <- c("twitter", "news", "blogs")
df
```

Most words are used very infrequently (<= .001% of the time).

# Histogram
```{r}
hist(wf_tbl$twitter$pct)
hist(wf_tbl$news$pct)
hist(wf_tbl$blogs$pct)
```



# Detecting non-English words
How many non-English words are there? I will want to remove these for the model,
so it will be important to figure out a method to detect them.

## Code non-English words
If a word fails a grep for the regex `[a-zA-Z0-9]` then it contains a character outside
the English alphabet (A to Z) or the number range 0-9. This is a decent way to detect
non-English words.

```{r}
for(i in 1:3){
  texts_tokenized[[i]] %>% mutate(english = grepl("^[a-zA-Z0-9]", word)) -> texts_tokenized[[i]]
}
```

```{r}
texts_tokenized[[1]] %>% filter(!english) %>% nrow / texts_tokenized[[1]] %>% nrow
texts_tokenized[[2]] %>% filter(!english) %>% nrow / texts_tokenized[[2]] %>% nrow
texts_tokenized[[3]] %>% filter(!english) %>% nrow / texts_tokenized[[3]] %>% nrow
```

Less than 1% of the words contain non-English, non-numeric characters. What are some examples?

```{r}
texts_tokenized[[1]] %>% filter(!english) %>% distinct(word) %>% select(word)
texts_tokenized[[2]] %>% filter(!english) %>% distinct(word) %>% select(word)
texts_tokenized[[3]] %>% filter(!english) %>% distinct(word) %>% select(word)
```

Most of these are not actually non-English words. Some are English words that
contain non-standard characters, like "Å“happiness". Later when cleaning the data for the model,
I can run a pass over the words and remove these characters.


# Reducing the memory footprint

To keep the memory / compute footprint down, I can try using a smaller subset of
each of the datasets. For the sake of testing this approach, I'll try taking a
random sample of the twitter set and comparing the frequencies with the original.

Specifically, I'll take a random subset of the lines (not the words!). Only
50,000 lines each.

## Randomly select a subset
```{r}
twitter_t[sample(nrow(twitter_t), 50000), ] -> twitter_t_subset
news_t[sample(nrow(news_t), 50000), ] -> news_t_subset
blogs_t[sample(nrow(blogs_t), 50000), ] -> blogs_t_subset
```

## Tokenize and count frequencies
```{r}
texts_tokenized_subset <- list('twitter' = twitter_t_subset %>% unnest_tokens(word, text), 
                        'news' = news_t_subset %>% unnest_tokens(word, text), 
                        'blogs' = blogs_t_subset %>% unnest_tokens(word, text))


wf_subset_tbl <- list('twitter' = texts_tokenized_subset$twitter %>% count(word, sort = TRUE) %>% mutate(pct = (n / sum(n)) * 100),
               'news' = texts_tokenized_subset$news %>% count(word, sort = TRUE) %>% mutate(pct = (n / sum(n)) * 100),
               'blogs' = texts_tokenized_subset$blogs %>% count(word, sort = TRUE) %>% mutate(pct = (n / sum(n)) * 100))
```

## Top 10 most common words
```{r}
kable(list(data.frame('twitter' = wf_subset_tbl$twitter[1:10,c(1,3)]),
           data.frame('news' = wf_subset_tbl$news[1:10,c(1,3)]),
           data.frame('blogs' = wf_subset_tbl$blogs[1:10,c(1,3)])))
```

Looking at this table, it appears the frequencies, stated in terms of percentage of total words
is similar to when I analyzed the full datasets. This suggests that an approach that uses
random sub-sampling to minimize the memory/compute requirements is reasonable.


# N-gram frequencies

It will be important to look at n-gram frequencies. In the context of text analysis, an n-gram is a 
sequence of words extracted from the text. N is the number of sequential words extracted.

For example, in the sentence "The boy kicked the ball" the following 2-grams can
be extracted: "the boy", "boy kicked", "kicked the", and "the ball"

```{r}
tibble(line = 1:length("The boy kicked the ball"), text = "The boy kicked the ball") %>% 
  unnest_tokens(word, text, token = "ngrams", n = 2)
```

Ngrams can be easily adapted for the task of prediction, as I will illustrate below.

This is also very memory intensive, much more than word tokenization because there 
are many more combinations. It's more than my puny computer can handle, so I'll 
use the random subset that I just created instead of the full dataset.

```{r}
texts_twograms <- list('twitter' = twitter_t_subset %>% unnest_tokens(word, text, token = "ngrams", n = 2), 
                        'news' = news_t_subset %>% unnest_tokens(word, text, token = "ngrams", n = 2), 
                        'blogs' = blogs_t_subset %>% unnest_tokens(word, text, token = "ngrams", n = 2))
```

```{r}
twogram_tbl <- list('twitter' = texts_twograms$twitter %>% count(word, sort = TRUE) %>% mutate(pct = (n / sum(n)) * 100),
               'news' = texts_twograms$news %>% count(word, sort = TRUE) %>% mutate(pct = (n / sum(n)) * 100),
               'blogs' = texts_twograms$blogs %>% count(word, sort = TRUE) %>% mutate(pct = (n / sum(n)) * 100))
```

## Top 10 most common 2-grams
```{r}
kable(list(data.frame('twitter' = twogram_tbl$twitter[1:10,c(1,3)]),
           data.frame('news' = twogram_tbl$news[1:10,c(1,3)]),
           data.frame('blogs' = twogram_tbl$blogs[1:10,c(1,3)])))
```

It can also be helpful to separate the ngrams so that each row presents each item
in separate columns.

```{r}
twogram_tbl$twitter %>%
    separate(word, c("word1", "word2"), sep = " ") -> texts_twograms_sep

texts_twograms_sep
```

For any given word, we can look at the most frequently occuring bigram pair. A simple
model could use this to predict the next most likely word to appear. For example,
we see that the word that most frequently appears after "I" is "love".

```{r}
texts_twograms_sep %>%
  filter(word1 == "i") %>%
  top_n(10, pct) %>%
  ggplot(aes(reorder(word2, pct), pct, fill = pct > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words following \"I\"") +
  ylab("% frequency of word following \"I\"") +
  coord_flip()
```


